{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe75baa",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a8ddf",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
    "\n",
    "## 1. Introduction: The Inner Monologue\n",
    "\n",
    "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
    "\n",
    "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering. \n",
    "\n",
    "### Why use a \"Dumb\" Model?\n",
    "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
    "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
    "\n",
    "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
    "\n",
    "### Visualizing the Process (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    Input[Question: 5+5*2?]\n",
    "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
    "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
    "    Step1 --> Step2[Step 2: 5+10=15]\n",
    "    Step2 --> Correct[Answer: 15 (Correct)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41e5ba",
   "metadata": {},
   "source": [
    "## 2. Concept: Latent Reasoning\n",
    "\n",
    "Why does this work?\n",
    "Because LLMs are \"Next Token Predictors\".\n",
    "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
    "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
    "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
    "\n",
    "**Writing is Thinking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba92b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\piyus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3088780",
   "metadata": {},
   "source": [
    "## 3. The Experiment: A Tricky Math Problem\n",
    "\n",
    "Let's try a problem that requires multi-step logic.\n",
    "\n",
    "**Problem:**\n",
    "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a70d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STANDARD (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
      "\n",
      "2 cans * 3 tennis balls per can = 6 tennis balls\n",
      "\n",
      "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
      "\n",
      "5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
    "\n",
    "# 1. Standard Prompt (Direct Answer)\n",
    "prompt_standard = f\"Answer this question: {question}\"\n",
    "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_standard).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b696ba6",
   "metadata": {},
   "source": [
    "### Critique\n",
    "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
    "\n",
    "Let's force it to think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dd65b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chain of Thought (Llama3.1-8b) ---\n",
      "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
      "\n",
      "1. Roger already has 5 tennis balls.\n",
      "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
      "3. Now, we add the tennis balls he already had (5) to the tennis balls he bought (6). 5 + 6 = 11\n",
      "\n",
      "So, Roger now has 11 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "# 2. CoT Prompt (Magic Phrase)\n",
    "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
    "\n",
    "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
    "print(llm.invoke(prompt_cot).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd205672",
   "metadata": {},
   "source": [
    "## 4. Analysis\n",
    "\n",
    "Look at the output. By explicitly breaking it down:\n",
    "1.  \"Roger starts with 5.\"\n",
    "2.  \"2 cans * 3 balls = 6 balls.\"\n",
    "3.  \"5 + 6 = 11.\"\n",
    "\n",
    "The model effectively \"debugs\" its own logic by generating the intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee779f",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1fa7c",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
    "\n",
    "## 1. Introduction: Beyond A -> B\n",
    "\n",
    "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
    "\n",
    "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4371aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~equests (C:\\Users\\piyus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\piyus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Using Llama3.1-8b\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a348d6",
   "metadata": {},
   "source": [
    "## 2. Tree of Thoughts (ToT)\n",
    "\n",
    "ToT explores multiple branches before making a decision. \n",
    "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
    "\n",
    "### Implementation\n",
    "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ea2d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tree of Thoughts (ToT) Result ---\n",
      "As a child psychologist, I would recommend **Solution 3: Create a \"Fruit and Veggie Face\" on their Plate** as the most sustainable approach. Here's why:\n",
      "\n",
      "1. **Encourages exploration and creativity**: This approach promotes curiosity and creativity in children, as they get to engage with the food and explore different shapes and colors.\n",
      "2. **Develops social-emotional skills**: Creating a \"face\" on the plate involves problem-solving, critical thinking, and fine motor skills, which are essential for social-emotional development.\n",
      "3. **Builds self-confidence**: When children create their own \"face\" on the plate, they feel a sense of accomplishment and pride in their work, which boosts self-confidence and self-esteem.\n",
      "4. **Increases food engagement**: By making mealtime more interactive and fun, children are more likely to engage with their food and try new vegetables.\n",
      "5. **No bribery involved**: This approach doesn't involve offering rewards or treats for eating vegetables, which can create unhealthy associations with food.\n",
      "6. **Promotes healthy eating habits**: By presenting vegetables in a fun and engaging way, children are more likely to develop healthy eating habits and a positive relationship with fruits and vegetables.\n",
      "7. **Fosters parent-child interaction**: This approach encourages parents to engage with their children during mealtime, promoting bonding and communication.\n",
      "\n",
      "While the other solutions have their merits, they may involve some form of bribery or external motivation, which can create an unhealthy relationship with food. For example, the \"Veggie Superhero\" approach may create a sense of obligation or responsibility, while the \"Rainbow Plate\" approach may be seen as a game or a challenge rather than a genuine experience.\n",
      "\n",
      "In contrast, the \"Fruit and Veggie Face\" approach is a more organic and sustainable way to promote healthy eating habits and a positive relationship with fruits and vegetables in children.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
    "\n",
    "# Step 1: The Branch Generator\n",
    "prompt_branch = ChatPromptTemplate.from_template(\n",
    "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
    ")\n",
    "\n",
    "branches = RunnableParallel(\n",
    "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
    "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
    "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Step 2: The Judge\n",
    "prompt_judge = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three proposed solutions for: '{problem}'\n",
    "    \n",
    "    1: {sol1}\n",
    "    2: {sol2}\n",
    "    3: {sol3}\n",
    "    \n",
    "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Chain: Input -> Branches -> Judge -> Output\n",
    "tot_chain = (\n",
    "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
    "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]}) \n",
    "    | prompt_judge\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
    "print(tot_chain.invoke(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38579cab",
   "metadata": {},
   "source": [
    "## 3. Graph of Thoughts (GoT)\n",
    "\n",
    "You asked: **\"Where is Graph of Thoughts?\"**\n",
    "\n",
    "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
    "\n",
    "### The Workflow (Writer's Room)\n",
    "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
    "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
    "3.  **Refine:** Polish the Master Plot.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "   Start(Concept) --> A[Draft 1]\n",
    "   Start --> B[Draft 2]\n",
    "   Start --> C[Draft 3]\n",
    "   A & B & C --> Mixer[Aggregator]\n",
    "   Mixer --> Final[Final Story]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "894940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Graph of Thoughts (GoT) Result ---\n",
      "\"Echoes of Eternity\" is a time-traveling epic that weaves together the intricate threads of science, love, and terror. Dr. Emma Taylor, a brilliant and reclusive scientist, has spent her entire career studying the anomalies of the human brain, and her latest discovery - a device capable of manipulating time and creating echoes of parallel universes - has the potential to change the course of history. However, as she uses the device to travel back to the 1920s to relive a pivotal moment from her grandmother's past, she finds herself falling deeply in love with a charming artist named Max, who is struggling to make a name for himself in a world on the brink of chaos. But their love is threatened by a mysterious figure from their own time, who has been tracking them across the ages and will stop at nothing to eliminate the echoes and prevent the timeline from collapsing, forcing Emma and Max to navigate the treacherous landscape of their own past and future to ensure a happy ending for themselves and the world they love.\n"
     ]
    }
   ],
   "source": [
    "# 1. The Generator (Divergence)\n",
    "prompt_draft = ChatPromptTemplate.from_template(\n",
    "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
    ")\n",
    "\n",
    "drafts = RunnableParallel(\n",
    "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
    "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
    "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# 2. The Aggregator (Convergence)\n",
    "prompt_combine = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    I have three movie ideas for the topic '{topic}':\n",
    "    1. Sci-Fi: {draft_scifi}\n",
    "    2. Romance: {draft_romance}\n",
    "    3. Horror: {draft_horror}\n",
    "    \n",
    "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
    "    Write one paragraph.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 3. The Chain\n",
    "got_chain = (\n",
    "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
    "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]}) \n",
    "    | prompt_combine\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
    "print(got_chain.invoke(\"Time Travel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cb724",
   "metadata": {},
   "source": [
    "## 4. Summary & Comparison Table\n",
    "\n",
    "| Method | Structure | Best For... | Cost/Latency |\n",
    "|--------|-----------|-------------|--------------|\n",
    "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
    "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
    "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High | \n",
    "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
    "\n",
    "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
