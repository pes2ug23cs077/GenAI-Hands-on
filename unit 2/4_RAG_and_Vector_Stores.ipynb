{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411ce1c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace94db8",
   "metadata": {},
   "source": [
    "# Unit 2: RAG, Vector Stores, and Indexing\n",
    "\n",
    "## Introduction\n",
    "LLMs have a knowledge cutoff and can hallucinate. **Retrieval Augmented Generation (RAG)** solves this by retrieving relevant data and injecting it into the prompt.\n",
    "\n",
    "In this notebook, we will master:\n",
    "1.  **Embeddings:** Representing text as vectors.\n",
    "2.  **Vector Stores:** Storing and searching vectors (FAISS).\n",
    "3.  **NaÃ¯ve RAG:** The standard Retrieval -> Augment -> Generate pipeline.\n",
    "4.  **Indexing Challenges:** Deep dive into how vector databases search efficiently (Flat, IVF, HNSW, PQ).\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4a: Embeddings & Vector Space\n",
    "\n",
    "### 1. Introduction: Computers Don't Read English\n",
    "\n",
    "If you ask a computer \"Is a cat similar to a dog?\", it doesn't know. To a computer, \"cat\" is just a string of bytes: `01100011...`.\n",
    "\n",
    "To solve this, we use **Embeddings**.\n",
    "\n",
    "### What is an Embedding?\n",
    "An embedding is a translation from **Words** to **Lists of Numbers (Vectors)**, such that similar words represent close numbers.\n",
    "\n",
    "### The Process (Flowchart)\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Input Text ('Apple')\"] -->|Tokenization| B[\"Tokens (101, 255)\"]\n",
    "    B -->|Embedding Model| C[\"Vector List ([0.1, -0.5, 0.9...])\"]\n",
    "    C -->|Store| D[\"Vector Database\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bd7eb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\piyus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\piyus\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Using a FREE, open-source model from Hugging Face\n",
    "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718acb1f",
   "metadata": {},
   "source": [
    "## 2. Viewing a Vector\n",
    "\n",
    "Let's see what the word \"Apple\" looks like to the machine.\n",
    "\n",
    "### Conceptual Note: Dimensions\n",
    "The vector below has **384 dimensions** (for MiniLM). \n",
    "- Imagine a graph with X and Y axes (2 Dimensions). You can plot a point (x, y).\n",
    "- Now imagine adding Z (3 Dimensions).\n",
    "- Now imagine **384 axes**. \n",
    "\n",
    "Each axis represents a feature (e.g., \"Is it a fruit?\", \"Is it red?\", \"Is it tech-related?\"). The numbers aren't random; they encode meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c67eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: 384\n",
      "First 5 numbers: [-0.0061384765431284904, 0.031011775135993958, 0.06479362398386002, 0.010941493324935436, 0.005267179571092129]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "vector = embeddings.embed_query(\"Apple\")\n",
    "\n",
    "print(f\"Dimensionality: {len(vector)}\")\n",
    "print(f\"First 5 numbers: {vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fd791",
   "metadata": {},
   "source": [
    "## 3. The Math: Cosine Similarity\n",
    "\n",
    "How do we know if two vectors are close? We measure the **Angle** between them.\n",
    "\n",
    "### Cosine Similarity Formula\n",
    "$$ \\text{Similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\n",
    "\n",
    "- **1.0**: Arrows point in the Exact Same Direction (Identical).\n",
    "- **0.0**: Arrows are Perpendicular (Unrelated).\n",
    "- **-1.0**: Arrows point in Opposite Directions (Opposite).\n",
    "\n",
    "**Experiment:** Let's compare \"Cat\", \"Dog\", and \"Car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58dd1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat vs Dog: 0.6606\n",
      "Cat vs Car: 0.4633\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "vec_cat = embeddings.embed_query(\"Cat\")\n",
    "vec_dog = embeddings.embed_query(\"Dog\")\n",
    "vec_car = embeddings.embed_query(\"Car\")\n",
    "\n",
    "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
    "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90b5f7",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "You should see that **Cat & Dog** score higher (e.g., ~0.8) than **Cat & Car** (e.g., ~0.3).\n",
    "This Mathematical Distance is the foundation of all Search engines and RAG systems.\n",
    "\n",
    "This is arguably the most important concept in modern AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6278ad2",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865ce8f",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 4b: Naive RAG Pipeline\n",
    "\n",
    "## 1. Introduction: The Open-Book Test\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is just an Open-Book Test architecture.\n",
    "1.  **Retrieval:** Find the right page in the textbook.\n",
    "2.  **Generation:** Write the answer using that page.\n",
    "\n",
    "### The Pipeline (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    User[User Question] --> Retriever[Retriever System]\n",
    "    Retriever -->|Search Database| Docs[Relevant Documents]\n",
    "    Docs --> Combiner[Prompt Template]\n",
    "    User --> Combiner\n",
    "    Combiner -->|Full Prompt w/ Context| LLM[Gemini Model]\n",
    "    LLM --> Answer[Final Answer]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dcd7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\piyus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Using the same free model as Part 4a\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f457ce",
   "metadata": {},
   "source": [
    "## 2. The \"Knowledge Base\" (Grounding)\n",
    "\n",
    "LLMs hallucinate because they rely on \"parametric memory\" (what they learned during training).\n",
    "RAG introduces \"non-parametric memory\" (external facts).\n",
    "\n",
    "Let's define some facts the LLM definitely *does not* know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "234faff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Piyush's favorite food is Pizza with extra cheese.\"),\n",
    "    Document(page_content=\"The secret password to the lab is 'Blueberry'.\"),\n",
    "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5812c6",
   "metadata": {},
   "source": [
    "## 3. Indexing ( Storing the knowledge)\n",
    "\n",
    "We use **FAISS** (Facebook AI Similarity Search) to store the embeddings.\n",
    "Think of FAISS as a super-fast librarian that organizes books by content, not title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1e1581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d826cf",
   "metadata": {},
   "source": [
    "## 4. The RAG Chain\n",
    "\n",
    "We use LCEL to stitch it together.\n",
    "\n",
    "**Step 1:** The `retriever` takes the question, converts it to numbers, and finds the closest document.\n",
    "**Step 2:** `RunnablePassthrough` holds the question.\n",
    "**Step 3:** The `prompt` combines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67a7a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret password to the lab is 'Blueberry'.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"\n",
    "Answer based ONLY on the context below:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"What is the secret password?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c34d42",
   "metadata": {},
   "source": [
    "## 5. Analysis\n",
    "\n",
    "The retrieval step is opaque here. In the next notebook (**4c**), we will look *inside* the retriever to understand how FAISS actually finds that document among millions of others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988faa1",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e4fe9",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 4c: Deep Dive into Indexing Algorithms\n",
    "\n",
    "## 1. Introduction: The Scale Problem\n",
    "\n",
    "Comparing 1 vector against 10 vectors is fast.\n",
    "Comparing 1 vector against **100 Million** vectors is slow.\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** was built to solve this.\n",
    "\n",
    "### The Trade-off Triangle\n",
    "You can pick 2:\n",
    "- **Speed** (Query time)\n",
    "- **Accuracy** (Recall)\n",
    "- **Memory** (RAM usage)\n",
    "\n",
    "We will explore algorithms that optimize different corners of this triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc999db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Mock Data: 10,000 vectors of size 128\n",
    "d = 128\n",
    "nb = 10000\n",
    "xb = np.random.random((nb, d)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4b504",
   "metadata": {},
   "source": [
    "## 2. Flat Index (Brute Force)\n",
    "\n",
    "**Concept:** Check every single item.\n",
    "\n",
    "- **Algo:** `IndexFlatL2`\n",
    "- **Pros:** 100% Accuracy (Gold Standard).\n",
    "- **Cons:** Slow (O(N)). Unusable at 1M+ vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "847ac87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat Index contains 10000 vectors\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(xb)\n",
    "print(f\"Flat Index contains {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e1ffc",
   "metadata": {},
   "source": [
    "## 3. IVF (Inverted File Index)\n",
    "\n",
    "**Concept:** Clustering / Partitioning.\n",
    "\n",
    "Imagine looking for a book. Instead of checking every shelf, you go to the \"Sci-Fi\" section. Then you only search books *in that section*.\n",
    "\n",
    "### How it works (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    Data[All 1M Vectors] -->|Train| Clusters[1000 Cluster Centers (Centroids)]\n",
    "    Query[User Query] -->|Step 1| FindClosest[Find Closest Centroid]\n",
    "    FindClosest -->|Step 2| Search[Search ONLY vectors in that Cluster]\n",
    "```\n",
    "\n",
    "**Analogy:** Voronoi Cells (Zip Codes). We only search the local zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "402817e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 100 # How many 'zip codes' (clusters) we want\n",
    "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "\n",
    "# We MUST train it first so it learns where the clusters are\n",
    "index_ivf.train(xb)\n",
    "index_ivf.add(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a28c5",
   "metadata": {},
   "source": [
    "## 4. HNSW (Hierarchical Navigable Small World)\n",
    "\n",
    "**Concept:** Six Degrees of Separation.\n",
    "\n",
    "Most data is connected. HNSW builds a **Graph**.\n",
    "- **Layer 0:** Every point connects to neighbors.\n",
    "- **Layer 1:** \"Express Highways\" connecting distant points.\n",
    "\n",
    "**Analogy:** Catching a flight.\n",
    "You don't fly Local -> Local -> Local.\n",
    "You fly Local -> **HUB** (Chicago) -> **HUB** (London) -> Local.\n",
    "\n",
    "- **Pros:** Extremely fast retrieval.\n",
    "- **Cons:** Heavier on RAM (needs to store the edges of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42c18025",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 16 # Number of connections per node (The 'Hub' factor)\n",
    "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
    "index_hnsw.add(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d63c1",
   "metadata": {},
   "source": [
    "## 5. PQ (Product Quantization)\n",
    "\n",
    "**Concept:** Compression (Lossy).\n",
    "\n",
    "Do we need 32-bit float precision (`0.123456789`)? No. `0.12` is fine.\n",
    "PQ breaks the vector into chunks and approximates them.\n",
    "\n",
    "**Analogy:** 4K Video vs 480p Video.\n",
    "- 480p is blurry, but it's 10x smaller and faster to stream.\n",
    "- Use PQ when you are RAM constrained (e.g., storing 1 Billion vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "598642ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ Compression complete. RAM usage minimized.\n"
     ]
    }
   ],
   "source": [
    "m = 8 # Split vector into 8 sub-vectors\n",
    "index_pq = faiss.IndexPQ(d, m, 8)\n",
    "index_pq.train(xb)\n",
    "index_pq.add(xb)\n",
    "print(\"PQ Compression complete. RAM usage minimized.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
