{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "411ce1c2",
      "metadata": {
        "id": "411ce1c2"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace94db8",
      "metadata": {
        "id": "ace94db8"
      },
      "source": [
        "# Unit 2: RAG, Vector Stores, and Indexing\n",
        "\n",
        "## Introduction\n",
        "LLMs have a knowledge cutoff and can hallucinate. **Retrieval Augmented Generation (RAG)** solves this by retrieving relevant data and injecting it into the prompt.\n",
        "\n",
        "In this notebook, we will master:\n",
        "1.  **Embeddings:** Representing text as vectors.\n",
        "2.  **Vector Stores:** Storing and searching vectors (FAISS).\n",
        "3.  **Naïve RAG:** The standard Retrieval -> Augment -> Generate pipeline.\n",
        "4.  **Indexing Challenges:** Deep dive into how vector databases search efficiently (Flat, IVF, HNSW, PQ).\n",
        "\n",
        "---\n",
        "\n",
        "## Part 4a: Embeddings & Vector Space\n",
        "\n",
        "### 1. Introduction: Computers Don't Read English\n",
        "\n",
        "If you ask a computer \"Is a cat similar to a dog?\", it doesn't know. To a computer, \"cat\" is just a string of bytes: `01100011...`.\n",
        "\n",
        "To solve this, we use **Embeddings**.\n",
        "\n",
        "### What is an Embedding?\n",
        "An embedding is a translation from **Words** to **Lists of Numbers (Vectors)**, such that similar words represent close numbers.\n",
        "\n",
        "### The Process (Flowchart)\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[\"Input Text ('Apple')\"] -->|Tokenization| B[\"Tokens (101, 255)\"]\n",
        "    B -->|Embedding Model| C[\"Vector List ([0.1, -0.5, 0.9...])\"]\n",
        "    C -->|Store| D[\"Vector Database\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface\n",
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxYVWM5pANts",
        "outputId": "bc6af8fa-48ba-4fa6-e542-c66878abbb20"
      },
      "id": "oxYVWM5pANts",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.2)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (1.2.13)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.6.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (9.1.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.16.0)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.62.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.2.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.12.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (9.1.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.6.2)\n",
            "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain_google_genai\n",
            "Successfully installed filetype-1.2.0 langchain_google_genai-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2bd7eb17",
      "metadata": {
        "id": "2bd7eb17"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a FREE, open-source model from Hugging Face\n",
        "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQUIzx9sABMH"
      },
      "id": "pQUIzx9sABMH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "718acb1f",
      "metadata": {
        "id": "718acb1f"
      },
      "source": [
        "## 2. Viewing a Vector\n",
        "\n",
        "Let's see what the word \"Apple\" looks like to the machine.\n",
        "\n",
        "### Conceptual Note: Dimensions\n",
        "The vector below has **384 dimensions** (for MiniLM).\n",
        "- Imagine a graph with X and Y axes (2 Dimensions). You can plot a point (x, y).\n",
        "- Now imagine adding Z (3 Dimensions).\n",
        "- Now imagine **384 axes**.\n",
        "\n",
        "Each axis represents a feature (e.g., \"Is it a fruit?\", \"Is it red?\", \"Is it tech-related?\"). The numbers aren't random; they encode meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6c67eb1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c67eb1c",
        "outputId": "3b3e90b5-1533-4e97-df47-46532b6fdf82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensionality: 384\n",
            "First 5 numbers: [-0.006138464901596308, 0.031011823564767838, 0.06479359418153763, 0.010941491462290287, 0.005267174914479256]\n"
          ]
        }
      ],
      "source": [
        "vector = embeddings.embed_query(\"Apple\")\n",
        "\n",
        "print(f\"Dimensionality: {len(vector)}\")\n",
        "print(f\"First 5 numbers: {vector[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0fd791",
      "metadata": {
        "id": "da0fd791"
      },
      "source": [
        "## 3. The Math: Cosine Similarity\n",
        "\n",
        "How do we know if two vectors are close? We measure the **Angle** between them.\n",
        "\n",
        "### Cosine Similarity Formula\n",
        "$$ \\text{Similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\n",
        "\n",
        "- **1.0**: Arrows point in the Exact Same Direction (Identical).\n",
        "- **0.0**: Arrows are Perpendicular (Unrelated).\n",
        "- **-1.0**: Arrows point in Opposite Directions (Opposite).\n",
        "\n",
        "**Experiment:** Let's compare \"Cat\", \"Dog\", and \"Car\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "58dd1396",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58dd1396",
        "outputId": "3a627cc5-c318-4abc-ad81-9c53c3e7e55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat vs Dog: 0.6606\n",
            "Cat vs Car: 0.4633\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "vec_cat = embeddings.embed_query(\"Cat\")\n",
        "vec_dog = embeddings.embed_query(\"Dog\")\n",
        "vec_car = embeddings.embed_query(\"Car\")\n",
        "\n",
        "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
        "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a90b5f7",
      "metadata": {
        "id": "5a90b5f7"
      },
      "source": [
        "### Analysis\n",
        "You should see that **Cat & Dog** score higher (e.g., ~0.8) than **Cat & Car** (e.g., ~0.3).\n",
        "This Mathematical Distance is the foundation of all Search engines and RAG systems.\n",
        "\n",
        "This is arguably the most important concept in modern AI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6278ad2",
      "metadata": {
        "id": "f6278ad2"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1865ce8f",
      "metadata": {
        "id": "1865ce8f"
      },
      "source": [
        "# Unit 2 - Part 4b: Naive RAG Pipeline\n",
        "\n",
        "## 1. Introduction: The Open-Book Test\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is just an Open-Book Test architecture.\n",
        "1.  **Retrieval:** Find the right page in the textbook.\n",
        "2.  **Generation:** Write the answer using that page.\n",
        "\n",
        "### The Pipeline (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    User[User Question] --> Retriever[Retriever System]\n",
        "    Retriever -->|Search Database| Docs[Relevant Documents]\n",
        "    Docs --> Combiner[Prompt Template]\n",
        "    User --> Combiner\n",
        "    Combiner -->|Full Prompt w/ Context| LLM[Gemini Model]\n",
        "    LLM --> Answer[Final Answer]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4dcd7e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dcd7e45",
        "outputId": "fa78daf3-cc1d-44e2-e76d-b320f7fd93d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Using the same free model as Part 4a\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f457ce",
      "metadata": {
        "id": "b6f457ce"
      },
      "source": [
        "## 2. The \"Knowledge Base\" (Grounding)\n",
        "\n",
        "LLMs hallucinate because they rely on \"parametric memory\" (what they learned during training).\n",
        "RAG introduces \"non-parametric memory\" (external facts).\n",
        "\n",
        "Let's define some facts the LLM definitely *does not* know."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "234faff9",
      "metadata": {
        "id": "234faff9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Piyush's favorite food is Pizza with extra cheese.\"),\n",
        "    Document(page_content=\"The secret password to the lab is 'Blueberry'.\"),\n",
        "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5812c6",
      "metadata": {
        "id": "2c5812c6"
      },
      "source": [
        "## 3. Indexing ( Storing the knowledge)\n",
        "\n",
        "We use **FAISS** (Facebook AI Similarity Search) to store the embeddings.\n",
        "Think of FAISS as a super-fast librarian that organizes books by content, not title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c1e1581d",
      "metadata": {
        "id": "c1e1581d"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d826cf",
      "metadata": {
        "id": "d5d826cf"
      },
      "source": [
        "## 4. The RAG Chain\n",
        "\n",
        "We use LCEL to stitch it together.\n",
        "\n",
        "**Step 1:** The `retriever` takes the question, converts it to numbers, and finds the closest document.\n",
        "**Step 2:** `RunnablePassthrough` holds the question.\n",
        "**Step 3:** The `prompt` combines them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "67a7a5ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a7a5ab",
        "outputId": "4b8a8ce5-a016-47e7-e051-492499a3b4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret password to the lab is 'Blueberry'.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"\n",
        "Answer based ONLY on the context below:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"What is the secret password?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c34d42",
      "metadata": {
        "id": "d4c34d42"
      },
      "source": [
        "## 5. Analysis\n",
        "\n",
        "The retrieval step is opaque here. In the next notebook (**4c**), we will look *inside* the retriever to understand how FAISS actually finds that document among millions of others."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b988faa1",
      "metadata": {
        "id": "b988faa1"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764e4fe9",
      "metadata": {
        "id": "764e4fe9"
      },
      "source": [
        "# Unit 2 - Part 4c: Deep Dive into Indexing Algorithms\n",
        "\n",
        "## 1. Introduction: The Scale Problem\n",
        "\n",
        "Comparing 1 vector against 10 vectors is fast.\n",
        "Comparing 1 vector against **100 Million** vectors is slow.\n",
        "\n",
        "**FAISS (Facebook AI Similarity Search)** was built to solve this.\n",
        "\n",
        "### The Trade-off Triangle\n",
        "You can pick 2:\n",
        "- **Speed** (Query time)\n",
        "- **Accuracy** (Recall)\n",
        "- **Memory** (RAM usage)\n",
        "\n",
        "We will explore algorithms that optimize different corners of this triangle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cc999db8",
      "metadata": {
        "id": "cc999db8"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Mock Data: 10,000 vectors of size 128\n",
        "d = 128\n",
        "nb = 10000\n",
        "xb = np.random.random((nb, d)).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c4b504",
      "metadata": {
        "id": "f0c4b504"
      },
      "source": [
        "## 2. Flat Index (Brute Force)\n",
        "\n",
        "**Concept:** Check every single item.\n",
        "\n",
        "- **Algo:** `IndexFlatL2`\n",
        "- **Pros:** 100% Accuracy (Gold Standard).\n",
        "- **Cons:** Slow (O(N)). Unusable at 1M+ vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "847ac87f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847ac87f",
        "outputId": "ddf71d66-1df8-4562-b50d-6b99e2dbc7b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat Index contains 10000 vectors\n"
          ]
        }
      ],
      "source": [
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(xb)\n",
        "print(f\"Flat Index contains {index.ntotal} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "507e1ffc",
      "metadata": {
        "id": "507e1ffc"
      },
      "source": [
        "## 3. IVF (Inverted File Index)\n",
        "\n",
        "**Concept:** Clustering / Partitioning.\n",
        "\n",
        "Imagine looking for a book. Instead of checking every shelf, you go to the \"Sci-Fi\" section. Then you only search books *in that section*.\n",
        "\n",
        "### How it works (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    Data[All 1M Vectors] -->|Train| Clusters[1000 Cluster Centers (Centroids)]\n",
        "    Query[User Query] -->|Step 1| FindClosest[Find Closest Centroid]\n",
        "    FindClosest -->|Step 2| Search[Search ONLY vectors in that Cluster]\n",
        "```\n",
        "\n",
        "**Analogy:** Voronoi Cells (Zip Codes). We only search the local zip code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "402817e5",
      "metadata": {
        "id": "402817e5"
      },
      "outputs": [],
      "source": [
        "nlist = 100 # How many 'zip codes' (clusters) we want\n",
        "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "\n",
        "# We MUST train it first so it learns where the clusters are\n",
        "index_ivf.train(xb)\n",
        "index_ivf.add(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0a28c5",
      "metadata": {
        "id": "4c0a28c5"
      },
      "source": [
        "## 4. HNSW (Hierarchical Navigable Small World)\n",
        "\n",
        "**Concept:** Six Degrees of Separation.\n",
        "\n",
        "Most data is connected. HNSW builds a **Graph**.\n",
        "- **Layer 0:** Every point connects to neighbors.\n",
        "- **Layer 1:** \"Express Highways\" connecting distant points.\n",
        "\n",
        "**Analogy:** Catching a flight.\n",
        "You don't fly Local -> Local -> Local.\n",
        "You fly Local -> **HUB** (Chicago) -> **HUB** (London) -> Local.\n",
        "\n",
        "- **Pros:** Extremely fast retrieval.\n",
        "- **Cons:** Heavier on RAM (needs to store the edges of the graph)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "42c18025",
      "metadata": {
        "id": "42c18025"
      },
      "outputs": [],
      "source": [
        "M = 16 # Number of connections per node (The 'Hub' factor)\n",
        "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
        "index_hnsw.add(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645d63c1",
      "metadata": {
        "id": "645d63c1"
      },
      "source": [
        "## 5. PQ (Product Quantization)\n",
        "\n",
        "**Concept:** Compression (Lossy).\n",
        "\n",
        "Do we need 32-bit float precision (`0.123456789`)? No. `0.12` is fine.\n",
        "PQ breaks the vector into chunks and approximates them.\n",
        "\n",
        "**Analogy:** 4K Video vs 480p Video.\n",
        "- 480p is blurry, but it's 10x smaller and faster to stream.\n",
        "- Use PQ when you are RAM constrained (e.g., storing 1 Billion vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "598642ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "598642ac",
        "outputId": "e9a1368d-c3d1-4ad8-b56e-cc975417a652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PQ Compression complete. RAM usage minimized.\n"
          ]
        }
      ],
      "source": [
        "m = 8 # Split vector into 8 sub-vectors\n",
        "index_pq = faiss.IndexPQ(d, m, 8)\n",
        "index_pq.train(xb)\n",
        "index_pq.add(xb)\n",
        "print(\"PQ Compression complete. RAM usage minimized.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDCoha9TAvz3"
      },
      "id": "EDCoha9TAvz3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}